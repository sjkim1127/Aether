/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export class Template {
    /**
     * Create a new template from content string.
     *
     * Use `{{AI:slot_name}}` syntax to define injection points.
     */
    constructor(content: string)
    /** Set the template name. */
    setName(name: string): void
    /** Add a slot with a prompt and optional temperature. */
    setSlot(name: string, prompt: string, temperature?: number | undefined | null): void
    /** Get all slot names in this template. */
    getSlotNames(): Array<string>
    /** Get the template content. */
    get content(): string
}
/** JavaScript-accessible RenderSession class for incremental rendering. */
export class RenderSession {
    /** Create a new empty render session. */
    constructor()
    /** Get the number of cached slot results. */
    get cachedCount(): number
    /** Clear all cached results. */
    clear(): void
}
/** JavaScript-accessible Slot class. */
export class Slot {
    /** Create a new slot. */
    constructor(name: string, prompt: string)
    /** Set the slot kind. */
    setKind(kind: string): void
    /** Set maximum lines constraint. */
    setMaxLines(max: number): void
    /** Set temperature for this slot. */
    setTemperature(temp: number): void
}
/** Main Aether engine for JavaScript. */
export class AetherEngine {
    /** Create a new engine with OpenAI provider. */
    static openai(model?: string | undefined | null): AetherEngine
    /** Create a new engine with Anthropic provider. */
    static anthropic(model?: string | undefined | null): AetherEngine
    /** Create a new engine with Gemini provider. */
    static gemini(model?: string | undefined | null): AetherEngine
    /** Create a new engine with Ollama provider. */
    static ollama(model?: string | undefined | null): AetherEngine
    /** Create a new engine with Grok provider. */
    static grok(model?: string | undefined | null): AetherEngine
    /** Set the project name for context. */
    setProject(project: string): void
    /** Set framework context. */
    setFramework(framework: string): void
    /** Set language context. */
    setLanguage(language: string): void
    /** Set global context. */
    setContext(project: string, language: string, framework: string): void
    /** Enable or disable self-healing. */
    setHeal(enable: boolean): void
    /** Enable or disable TOON protocol. */
    setToon(enable: boolean): void
    /** Enable or disable semantic caching. */
    setCache(enable: boolean): void
    /** Set maximum retry count for generation. */
    setMaxRetries(count: number): void
    /** Render a template with AI-generated code. */
    render(template: Template): Promise<string>
    /**
     * Render a template incrementally using a session to cache results.
     *
     * Only slots that have changed since the last render will be regenerated.
     */
    renderIncremental(template: Template, session: RenderSession): Promise<string>
    /**
     * Get streaming chunks as an array (alternative to callback-based streaming).
     * Returns an array of strings, each representing a chunk of the generated content.
     */
    getStreamChunks(template: Template, slotName: string): Promise<Array<string>>
    /** Execute a Rhai script directly. */
    executeScript(script: string, inputs?: Record<string, any> | undefined | null): string
}
/** One-line code generation with a simple prompt. */
export function generate(prompt: string): Promise<string>
/** One-line template rendering with simple slot mapping. */
export function renderTemplate(template: string, slots: Record<string, string>): Promise<string>
export enum ProviderType {
    OpenAI = 'OpenAI',
    Anthropic = 'Anthropic',
    Ollama = 'Ollama',
    Gemini = 'Gemini',
    Grok = 'Grok'
}
export interface ProviderConfig {
    apiKey: string
    model: string
    apiKeyUrl?: string
}
